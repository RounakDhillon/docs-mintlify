---
title: Run the VertexAI Connector Externally
description: Use YAML to ingest VertexAI metadata including models, inputs, tags, and monitoring logs.
slug: /connectors/ml-model/vertexai/yaml
collate: true
---

import IngestionCli from '/snippets/connectors/yaml/ingestion-cli.mdx'

import PythonRequirements from '/snippets/connectors/python-requirements.mdx'

import ExternalIngestionDeployment from '/snippets/connectors/external-ingestion-deployment.mdx'

import IngestionSink from '/snippets/connectors/yaml/ingestion-sink.mdx'

import WorkflowConfig from 'snippets/connectors/yaml/workflow-config.mdx'

import SourceConfig from 'snippets/connectors/yaml/database/source-config.mdx'

import GcpConfig from 'snippets/connectors/yaml/common/gcp-config.mdx'

import GcpConfigDef from 'snippets/connectors/yaml/common/gcp-config-def.mdx'

{% connectorDetailsHeader
name="VertexAI"
stage="BETA"
platform="Collate"
availableFeatures=["ML Store", "ML Features", "Hyper parameters"]
unavailableFeatures=[]
/ %}

In this section, we provide guides and references to use the VertexAI connector.

Configure and schedule VertexAI metadata from the OpenMetadata UI:

- [Requirements](#requirements)
- [Metadata Ingestion](#metadata-ingestion)

<ExternalIngestionDeployment />

## Requirements

### Python Requirements

<PythonRequirements />

To run the VertexAI ingestion, you will need to install:

```bash
pip3 install "openmetadata-ingestion[vertexai]"
```

### GCP Permissions

To execute metadata extraction workflow successfully the user or the service account should have enough access to fetch required data. Following table describes the minimum required permissions

{% multiTablesWrapper %}

| #    | GCP Permission                | Required For            |
| :--- | :---------------------------- | :---------------------- |
| 1    | aiplatform.models.get         | Metadata Ingestion      |
| 2    | aiplatform.models.list        | Metadata Ingestion      |


{% /multiTablesWrapper %}


## Metadata Ingestion

### 1. Define the YAML Config

This is a sample config for VertexAI:

{% codePreview %}

{% codeInfoContainer %}

#### Source Configuration - Service Connection

{% codeInfo srNumber=1 %}

**credentials**: 
You can authenticate with your vertexai instance using either `GCP Credentials Path` where you can specify the file path of the service account key or you can pass the values directly by choosing the `GCP Credentials Values` from the service account key file.

You can checkout [this](https://cloud.google.com/iam/docs/keys-create-delete#iam-service-account-keys-create-console) documentation on how to create the service account keys and download it.

**gcpConfig:**

**1.** Passing the raw credential values provided by VertexAI. This requires us to provide the following information, all provided by VertexAI:

{% /codeInfo %}

<GcpConfigDef />

{% codeInfo srNumber=4 %}

**2.**  Passing a local file path that contains the credentials:
  - **gcpCredentialsPath**

**Location**:
Location refers to the geographical region where your resources, such as datasets, models, and endpoints, are physically hosted.(e.g. `us-central1`, `europe-west4`)

{% /codeInfo %}

{% /codeInfoContainer %}

{% codeBlock fileName="filename.yaml" %}

```yaml {% isCodeBlock=true %}
source:
  type: vertexai
  serviceName: localvx
  serviceConnection:
    config:
        type: VertexAI
```
```yaml {% srNumber=1 %}
        credentials:
            gcpConfig:
```

<GcpConfig />

```yaml {% srNumber=4 %}
        location: PROJECT LOCATION/REGION (us-central1)
```
```yaml {% srNumber=2 %}
        # connectionOptions:
        #   key: value
```
```yaml {% srNumber=3 %}
        # connectionArguments:
        #   key: value
```

<SourceConfig />

<IngestionSink />

<WorkflowConfig />

{% /codeBlock %}

{% /codePreview %}

<IngestionCli />
